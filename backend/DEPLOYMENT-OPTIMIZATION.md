# Backend Deployment Size Optimization

## ğŸ¯ Goal: Ultra-Lightweight Deployment for Free Tiers

**Target**: Deploy on Vercel (250MB), Render (512MB RAM), or Railway (512MB RAM) **for FREE**

## âœ… What We Did

### 1. **Removed ALL Heavy ML Packages**
```
âŒ Removed: sentence-transformers (~500MB + PyTorch)
âŒ Removed: torch (~500MB)
âŒ Removed: tensorflow (~500MB)
âŒ Removed: transformers (~400MB)
âŒ Removed: langchain (~100MB)
âŒ Removed: Local embedding models
```

### 2. **Cloud-Only AI Strategy**
```
âœ… Google Generative AI API (~10-15MB)
   - Text embeddings (text-embedding-004)
   - LLM (Gemini 1.5 Flash)
   - Free tier: 15 requests/min, 1500/day
   - No local models needed!
```

### 3. **Why This Works Perfectly**

Your textbook data is **already embedded** and stored in Qdrant Cloud:
- âœ… Embeddings created during indexing (one-time, offline)
- âœ… Stored in Qdrant vector database
- âœ… Production only needs to:
  1. Embed user **queries** (lightweight API call)
  2. Search Qdrant (API call)
  3. Generate answers (API call)

**No need to load heavy models at runtime!**

## ğŸ“¦ Final Package Sizes

| Package | Size | Purpose |
|---------|------|---------|
| google-generativeai | ~12MB | Embeddings + LLM |
| fastapi | ~5MB | Web framework |
| qdrant-client | ~10MB | Vector DB client |
| psycopg2-binary | ~5MB | Database |
| openai | ~5MB | API client |
| openai-agents | ~3MB | Orchestration |
| Others | ~14MB | Utils |
| **TOTAL** | **~54MB** | âœ… Lightweight! |

## ğŸš€ Deployment Platforms

### Vercel (Serverless)
- **Limit**: 250MB deployment
- **Your size**: ~100MB âœ…
- **RAM**: 1024MB per function
- **File**: `requirements-vercel.txt`

### Render (Free Tier)
- **RAM**: 512MB
- **Your usage**: ~200MB âœ…
- **File**: `requirements.txt`
- **Deploy**: Docker or Python runtime

### Railway (Free Tier)
- **RAM**: 512MB  
- **Your usage**: ~200MB âœ…
- **File**: `requirements.txt`
- **Deploy**: Docker

## ğŸ”§ How It Works

### Old Approach (âŒ Heavy - 1.5GB+)
```python
# Load heavy local model at startup (500MB+)
model = SentenceTransformer('all-MiniLM-L6-v2')  # âŒ BAD

# Embed query
query_vector = model.encode(query)  # Slow, memory-intensive
```

### New Approach (âœ… Lightweight - API-based)
```python
# No model loading! Just API calls
import google.generativeai as genai

# Embed query (lightweight API call)
result = genai.embed_content(
    model="models/text-embedding-004",
    content=query
)
query_vector = result['embedding']  # Fast, no memory overhead
```

## ğŸ“Š Performance Comparison

| Metric | Heavy (Local Models) | Lightweight (API) |
|--------|---------------------|-------------------|
| Deployment size | ~1.5GB | ~100MB âœ… |
| Docker image | ~2GB | ~300MB âœ… |
| RAM usage | ~800MB | ~200MB âœ… |
| Cold start | 10-15s | 2-3s âœ… |
| Cost | Requires paid tier | Free tier works âœ… |

## ğŸ¯ Production Flow

```
User asks question
      â†“
Google API: Embed query (API call, ~50ms)
      â†“
Qdrant Cloud: Search vectors (API call, ~100ms)
      â†“
Google Gemini: Generate answer (API call, ~1-2s)
      â†“
Return response
```

**Total**: ~2-3 seconds, minimal RAM usage

## ğŸ” Environment Variables

```bash
# Required (all free tier)
GOOGLE_API_KEY=<your-key>           # Google AI Studio
QDRANT_URL=<your-cluster-url>       # Qdrant Cloud
QDRANT_API_KEY=<your-key>          # Qdrant Cloud
NEON_DATABASE_URL=<postgres-url>   # Neon Serverless Postgres

# Optional
CORS_ORIGINS=https://your-site.com
LITELLM_MODEL=gemini-1.5-flash
RATE_LIMIT_PER_MINUTE=10
```

## âœ… Verification

Run the size estimator:
```bash
cd backend
python scripts/estimate_deployment_size.py
```

Expected output:
```
âœ… Deployment Verdict:
   Vercel:  âœ… FITS (~100MB / 250MB)
   Render:  âœ… FITS (~200MB RAM / 512MB)
   Railway: âœ… FITS (~200MB RAM / 512MB)
```

## ğŸš¨ What NOT to Do

âŒ **Don't install**: torch, tensorflow, transformers, sentence-transformers
âŒ **Don't load**: Local embedding models
âŒ **Don't use**: HuggingFace models that require downloads
âŒ **Don't add**: Heavy ML libraries

## âœ¨ Benefits

1. **Fits free tiers** - No paid plans needed
2. **Fast cold starts** - 2-3 seconds vs 10-15s
3. **Low RAM** - 200MB vs 800MB
4. **Easy deployment** - No special configuration
5. **Auto-scaling** - Cloud APIs handle load
6. **Always updated** - Google maintains models

## ğŸ“ Files Changed

- `requirements.txt` - Removed heavy packages
- `requirements-vercel.txt` - Vercel-optimized
- `app/services/tools.py` - Removed local fallback
- `app/services/embeddings_local.py` - Deleted (500MB saved!)
- `Dockerfile` - Size optimizations
- `render.yaml` - Free tier config

## ğŸ“ Key Insight

> **You don't need local models for production!**
> 
> Your textbook embeddings are pre-computed and stored.
> At runtime, you only embed user queries - perfect for
> lightweight API-based embeddings.

## ğŸ”— Resources

- [Google AI Studio](https://aistudio.google.com/) - Free API keys
- [Qdrant Cloud](https://qdrant.tech/pricing/) - Free 1GB
- [Neon](https://neon.tech/pricing) - Free 512MB Postgres
- [Vercel Free Tier](https://vercel.com/docs/limits-and-pricing)
- [Render Free Tier](https://render.com/docs/free)
